# Reinforcement Learning from Human Feedback (RLHF)

> **Version:** 1.0.0
> **Status:** active
> **Last Updated:** 2026-02-08
> **Provenance:** 3P
> **Derives From:** —

Training and aligning AI systems using human preference signals to improve output quality and safety. Adopted in SemOps for the concept promotion workflow where human-in-the-loop feedback curates knowledge.

---

## Source

- **Standard/Origin:** OpenAI / DeepMind research
- **URL:** N/A

## Adoption Context

- **Original Capability:** `style-learning` — improving AI output quality with human feedback
- **Problem Statement:** AI-generated content quality improves with structured human feedback. Without a feedback loop, AI outputs plateau at their base capability level.
- **Selection Criteria:** Reinforcement Learning from Human Feedback (RLHF) methodology. Applied to style capture in semops-publisher — human editorial corrections train the system to match authorial voice and quality standards.
- **ADR:** —

## Related Patterns

- `seci`, `scale-projection`

